# EKS Cluster with Karpenter (Spot + Graviton Support) â€” Terraform POC

This project is a proof-of-concept for automating the deployment of an AWS EKS cluster using Terraform. It includes a setup for Karpenter as a cluster autoscaler with support for both x86 and arm64 (Graviton) architectures using Spot instances. It also contains a cloud architecture proposal for a fictional startup, *Innovate Inc.*, designed to be scalable, secure, and cloud-native.

---

## ðŸ“¦ Project Structure

```
OpsFleet_technical_task/
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ backend.tf
â”‚   â”‚   â”œâ”€â”€ dev.tfvars
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ old.backend-dev.hcl.notinuse
â”‚   â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”‚   â”œâ”€â”€ provider.tf
â”‚   â”‚   â”œâ”€â”€ terraform.tfstate
â”‚   â”‚   â”œâ”€â”€ terraform.tfstate.1750763648.backup
â”‚   â”‚   â”œâ”€â”€ terraform.tfstate.backup
â”‚   â”‚   â”œâ”€â”€ tfplan
â”‚   â”‚   â”œâ”€â”€ tfplan.vpc
â”‚   â”‚   â””â”€â”€ variables.tf
â”‚   â””â”€â”€ eks-providers.tf
â”œâ”€â”€ karpenter-provisioners/
â”‚   â”œâ”€â”€ nodeclass.yaml
â”‚   â”œâ”€â”€ provisioner-arm64-spot.yaml
â”‚   â””â”€â”€ provisioner-x86-spot.yaml
â”œâ”€â”€ lock-policy.json
â”œâ”€â”€ Makefile
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ eks/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”‚   â””â”€â”€ variables.tf
â”‚   â”œâ”€â”€ karpenter/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ versions.tf
â”‚   â””â”€â”€ vpc/
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ outputs.tf
â”‚       â””â”€â”€ variables.tf
â”œâ”€â”€ README.md
â””â”€â”€ terraform.tfstate.d/
    â””â”€â”€ dev/

```

---

## ðŸš€ Getting Started

# OpsFleetâ€¯â€“â€¯EKSâ€¯+â€¯Karpenter reference environment
> Terraform 1.8 Â· AWS provider â‰¥5 Â· EKS module 20.x Â· Karpenter 0.37  
> Author: Daniel Schmidt

---

## 0.  Quick diagram

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  developer â”‚â”€â”€tfâ†’ VPC â”€â”¼â”€â–º  EKS   â”œâ”€â”€â”€â–º   core nodeâ€‘group (tiny, onâ€‘demand)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
        â–²                                          Helm (IRSA)
        â”‚                                              â”‚
        â”‚                                        Karpenter
        â”‚                                              â–¼
        â”‚                                   Spot & onâ€‘demand capacity
        â”‚
        â””â”€â”€Â GitHub ActionsÂ /Â Atlantis (optional)
```

---

## 1.  Prerequisites

| item | version / notes |
|------|-----------------|
| Terraform CLI | **1.8.x**\* |
| AWS CLI       | â‰¥Â 2.15 |
| `aws` profile | `opsfleet-dev` with the *candidatesâ€‘restricted* policy shown below **plus** the two additions noted in Â§1.1 |
| kubectl       | matches the EKS minor (v1.32) |
| helm          | â‰¥Â 3.15 |

\*The provider lockâ€‘files (`.terraform.lock.hcl`) are committed; a higher TFÂ CLI works too.

### 1.1  Temporary IAM additions for the lab user

Add these two statements (or detach the managed policies) **before any `terraform apply`**:

```json
{
  "Effect": "Allow",
  "Action": "iam:CreateOpenIDConnectProvider",
  "Resource": "*"
},
{
  "Effect": "Allow",
  "Action": "eks:AssociateAccessPolicy",
  "Resource": "*"
}
```

> The first is needed by the `terraform-aws-modules/eks`Â module when `enable_irsa = true`.  
> The second allows the oneâ€‘liner that maps your IAM user toÂ `system:masters` via the new EKSÂ â€œAccessÂ Entryâ€ API.

Remove them afterwards if your security restrictions require.

---

## 2.  Oneâ€‘time bootstrap

```bash
# clone, thenâ€¦
make init           # terraform init + provider/version checks
aws sts get-caller-identity --profile opsfleet-dev  # sanity
```

---

## 3.  Recommended apply workflow (moduleâ€‘byâ€‘module)

> Trying to run **everything** in a single `terraform apply` often fails on fresh
> accounts because Karpenter needs the cluster and the OIDC provider *first*.

| step | command | rationale |
|------|---------|-----------|
| **3â€‘a** | `terraform -chdir=environments/dev apply -var-file=dev.tfvars -target=module.vpc` | carve the VPC & subnets; safe & idempotent |
| **3â€‘b** | `terraform â€¦ apply -target=module.eks` | builds the cluster + tiny `core` nodeâ€‘group & core addâ€‘ons |
| **3â€‘c** | **map yourself toÂ `system:masters`**<br>```bash\naws eks associate-access-policy \\\n  --profile opsfleet-dev \\\n  --cluster-name opsfleet-eks-dev-us-east-1 \\\n  --principal-arn arn:aws:iam::851725384896:user/daniel_schmidt \\\n  --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy \\\n  --access-scope type=cluster\n``` | new (2024â€‘11) EKS *AccessÂ Entry* API â€“ succeeds in seconds |
| **3â€‘d** | `aws eks update-kubeconfig --profile opsfleet-dev --name opsfleet-eks-dev-us-east-1`<br>`kubectl auth can-i '*' '*'` | verify we are clusterâ€‘admin |
| **3â€‘e** | `terraform â€¦ apply -target=module.karpenter` | IRSA role âžœ namespace âžœ Helm release |
| **3â€‘f** | `kubectl -n karpenter rollout status deploy/karpenter`<br>`kubectl -n karpenter logs -l app.kubernetes.io/name=karpenter --tail=20` | ensure it has valid STS creds; if **AccessDenied** â†’ doubleâ€‘check the role trust JSON (below) |

Once all three modules are healthy you can drop the `-target` flags and run
`make plan` / `make apply` as usual.

---

## 4.  IRSA trustÂ â€‘ reference JSON

Every time Terraform runs it renders this into  
`aws_iam_role.karpenter.assume_role_policy`:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::<ACCOUNT_ID>:oidc-provider/oidc.eks.<REGION>.amazonaws.com/id/<OIDC_ID>"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "oidc.eks.<REGION>.amazonaws.com/id/<OIDC_ID>:sub": "system:serviceaccount:karpenter:karpenter"
        }
      }
    }
  ]
}
```

If Karpenter crashes with `WebIdentityErr` **after** this trust policy is in
place, the usual culprit is a race: delete the failing pods once and they will
restart with fresh tokens:

```bash
kubectl -n karpenter delete pod -l app.kubernetes.io/name=karpenter
```

---

## 5.  Makefile targets

| target | does |
|--------|------|
| `make init` | `terraform init` for `./environments/dev` |
| `make plan` | `terraform plan -var-file=dev.tfvars` |
| `make apply` | **assumes the threeâ€‘step bootstrap already done**; runs a full apply |
| `make destroy` | tears down the dev environment (VPC remains for speed) |

---

## 6.  Troubleshooting checklist

1. **Nodeâ€‘group stuck** â†’ Check subnets are private *and* have route to a NAT; verify that the AMI matches CPU arch (`t4g.medium` =Â ARM64).
2. **`Unsupported authentication mode update`** â†’ never set `access_config.authentication_mode` manually; let the module manage it.
3. **Karpenter pending** â†’ make sure at least one EC2 instance family in your provisioners is allowed by the AWS accountâ€™s serviceâ€‘quotas and AZâ€‘availability.

---

## 7.  Cleaning up

```bash
make destroy               # modules
aws eks delete-cluster â€¦   # if Terraform lost state
bash nuke-account.sh       # â˜¢  danger â€“ removes **all** resources in the account
```

---

## 8.Quick demo

```bash
# provision spot pools
kubectl apply -f karpenter-provisioners/

# run one pod on Graviton, one on x86
kubectl apply -f test-arm64-and-x86-spot.yaml

--

*(The remainder of the original README â€“ design goals, module authors,
licence â€“ stays unchanged below this line.)*
